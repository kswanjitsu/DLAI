{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/karl/anaconda3/envs/rapids-21.08/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "WARNING:tensorflow:From /home/karl/PycharmProjects/DLAI/translate/labeler.py:148: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/karl/anaconda3/envs/rapids-21.08/lib/python3.7/site-packages/tensorflow/python/ops/rnn.py:447: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/karl/anaconda3/envs/rapids-21.08/lib/python3.7/site-packages/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py:981: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/karl/anaconda3/envs/rapids-21.08/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:201: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/karl/anaconda3/envs/rapids-21.08/lib/python3.7/site-packages/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py:903: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  warnings.warn(\"`tf.nn.rnn_cell.LSTMCell` is deprecated and will be \"\n",
      "/home/karl/anaconda3/envs/rapids-21.08/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1727: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  warnings.warn('`layer.add_variable` is deprecated and '\n",
      "/home/karl/anaconda3/envs/rapids-21.08/lib/python3.7/site-packages/tensorflow/python/keras/legacy_tf_layers/core.py:171: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  warnings.warn('`tf.layers.dense` is deprecated and '\n",
      "/home/karl/anaconda3/envs/rapids-21.08/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1719: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n",
      "2021-10-03 11:38:41.119223: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-10-03 11:38:41.121515: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "2021-10-03 11:38:41.144395: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:196] None of the MLIR optimization passes are enabled (registered 0 passes)\n",
      "2021-10-03 11:38:41.179283: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3793005000 Hz\n"
     ]
    }
   ],
   "source": [
    "# Import CWI modules and point machine path to temporary path that the CWI module uses to do work\n",
    "from complex_labeller import Complexity_labeller\n",
    "model_path = './cwi_seq.model'\n",
    "temp_path = './temp_file.txt'\n",
    "model = Complexity_labeller(model_path, temp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2\n",
      "For maximum performance, you can install NMSLIB from sources \n",
      "pip install --no-binary :all: nmslib\n",
      "/home/karl/anaconda3/envs/rapids-21.08/lib/python3.7/site-packages/spacy/util.py:732: UserWarning: [W095] Model 'en_core_sci_scibert' (0.4.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.1.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "/home/karl/anaconda3/envs/rapids-21.08/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/karl/anaconda3/envs/rapids-21.08/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<scispacy.linking.EntityLinker at 0x7f7e60d3fb50>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import spaCy NLP modules and libraries\n",
    "import spacy\n",
    "spacy.require_gpu()\n",
    "import scispacy\n",
    "from scispacy.linking import EntityLinker\n",
    "nlp_med = spacy.load(\"en_core_sci_scibert\")\n",
    "nlp_med.add_pipe(\"scispacy_linker\", config={\"resolve_abbreviations\": True, \"linker_name\": \"umls\"})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "<readability_v_ks.ReadabilityComponent at 0x7f7c9534cf90>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import custom readability scoring modules for spacy\n",
    "import readability_v_ks\n",
    "nlp_read = readability_v_ks.spacy.load(\"en_core_web_sm\")\n",
    "nlp_read.add_pipe('readability')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Pandas deserves its own import cell - multiple packages rely on it\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Import TFIDF modules\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Import GingerIt modules\n",
    "from gingerit.gingerit import GingerIt # pip install gingerit\n",
    "#import pandas as pd\n",
    "import pysbd, re # pip install pysbd\n",
    "segmentor = pysbd.Segmenter(language=\"en\", clean=False)\n",
    "subsegment_re = r'[^;:\\n•]+[;,:\\n•]?\\s*'\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# The main function of this script which calls many functions below to \"automatically translate\" medical text\n",
    "def automatic_translation(document, new_document, find_complex_words, sub_umls_hypernym, semantic_df):\n",
    "    cw_list = find_complex_words(document)\n",
    "    new_document = sub_umls_hypernym(document, new_document, semantic_df, cw_list)\n",
    "    return new_document, cw_list\n",
    "\n",
    "#tui_word\n",
    "#token\n",
    "#token_string_list\n",
    "#test_document = document\n",
    "#new_document = new_document.lower()\n",
    "#new_document = runGinger(new_document)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Function the finds complex word in a sentence with the CWI module from the 2019 paper\n",
    "def find_complex_words(test_document):\n",
    "    Complexity_labeller.convert_format_string(model, test_document)\n",
    "    dataframe = Complexity_labeller.get_dataframe(model)\n",
    "\n",
    "    # Access binary labeling information from the dataframe format:\n",
    "    cw_list = list(zip(dataframe['sentences'].values[0], dataframe['labels'].values[0], dataframe['probs'].values[0]))\n",
    "\n",
    "    # get_bin_labels returns the binary complexity labels for the input\n",
    "    # bin_label_list = Complexity_labeller.get_bin_labels(model)\n",
    "\n",
    "    # The `get_prob_labels` method returns the probability of each token belonging to the complex class.\n",
    "    # prob_label_list = Complexity_labeller.get_prob_labels(model)\n",
    "    return cw_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Function to substitute out complex words with hypernyms from UMLS semantic tree - we need to fix this to use hypernymy look up table once complete\n",
    "# This requires the sentence to have first been passed through the \"find_complex_words\" function as it needs the complex words list aka cw_list\n",
    "def sub_umls_hypernym(document, new_document, semantic_df, cw_list):\n",
    "    doc_med = nlp_med(document)\n",
    "    linker = nlp_med.get_pipe(\"scispacy_linker\")\n",
    "    for token in doc_med.ents:\n",
    "        token_string = str(token)\n",
    "        token_string_list = token_string.split(' ')\n",
    "        tui = ''\n",
    "\n",
    "        try:\n",
    "            umls_data = linker.kb.cui_to_entity[token._.kb_ents[0][0]]\n",
    "            umls_data = str(umls_data).split('\\n')\n",
    "            tui = umls_data[2]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            tui_split = tui.split(' ')\n",
    "            for tui in tui_split:\n",
    "                try:\n",
    "                    tui = tui.strip(',')\n",
    "                    tui_word_row = semantic_df.loc[semantic_df['TUI_ID'] == tui] # pandas sem df find row with matching tui\n",
    "                    tui_word = tui_word_row.iloc[0,0] # tui_word is a single row 3 column df, select row 0 (only row), column 0 (word)\n",
    "                    #tui_tree_level = tui_word_row.iloc[0,2] # same as above but the tree level is in third cell, aka index 2, of each row\n",
    "                    tui_word = str(tui_word).lstrip()\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "        except IndexError or UnboundLocalError:\n",
    "            pass\n",
    "\n",
    "        for i in cw_list:\n",
    "            cw = i[0]\n",
    "            cw_bin_complexity = i[1]\n",
    "\n",
    "            if cw in token_string_list and cw_bin_complexity == 1:\n",
    "                try:\n",
    "                    if tui_word != '':\n",
    "                        new_document = new_document.replace(str(token), tui_word.lower())\n",
    "                except:\n",
    "                    pass\n",
    "    return new_document"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Call the functions from the customized readability script Saadh and I wrote\n",
    "def grade_the_documents(document, new_document):\n",
    "    doc_med = nlp_read(document)\n",
    "    og_fk_score = str(doc_med._.flesch_kincaid_reading_ease)\n",
    "    og_fk_grade = str(doc_med._.flesch_kincaid_grade_level)\n",
    "    og_dc_score = str(doc_med._.dale_chall)\n",
    "    og_smog_score = str(doc_med._.smog)\n",
    "    og_cl_score = str(doc_med._.coleman_liau_index)\n",
    "    og_ar_index = str(doc_med._.automated_readability_index)\n",
    "    og_forcast = str(doc_med._.forcast)\n",
    "\n",
    "    #new_document = new_document['result'].replace('_',' ')\n",
    "    doc_new = nlp_read(new_document)\n",
    "    new_fk_score = str(doc_new._.flesch_kincaid_reading_ease)\n",
    "    new_fk_grade = str(doc_new._.flesch_kincaid_grade_level)\n",
    "    new_dc_score = str(doc_new._.dale_chall)\n",
    "    #new_smog_score = str(doc_new._.smog)\n",
    "    new_cl_score = str(doc_new._.coleman_liau_index)\n",
    "    new_ar_index = str(doc_new._.automated_readability_index)\n",
    "    #new_forcast = str(doc_new._.forcast)\n",
    "    return og_fk_score, new_fk_score, og_fk_grade, new_fk_grade, og_dc_score, new_dc_score, og_cl_score, new_cl_score, og_ar_index, new_ar_index"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Grammar checker - we use GingerIt - this function requires splitting the sentence up as it is the free API version\n",
    "def runGinger(par):\n",
    "    fixed = []\n",
    "    for sentence in segmentor.segment(par):\n",
    "        if len(sentence) < 300:\n",
    "            fixed.append(GingerIt().parse(sentence)['result'])\n",
    "        else:\n",
    "            subsegments = re.findall(subsegment_re, sentence)\n",
    "            if len(subsegments) == 1 or any(len(v) < 300 for v in subsegments):\n",
    "                fixed.append(sentence)\n",
    "            else:\n",
    "                res = []\n",
    "                for s in subsegments:\n",
    "                    res.append(GingerIt().parse(s)['result'])\n",
    "                fixed.append(\"\".join(res))\n",
    "    return \" \".join(fixed)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "import difflib as dl\n",
    "def sentencediff(sent1, sent2):\n",
    "    word_list = []\n",
    "    s1 = sent1.split(' ')\n",
    "    s2 = sent2.split(' ')\n",
    "    dl.context_diff(s1, s2)\n",
    "    for diff in dl.context_diff(s1, s2):\n",
    "        if diff.startswith('!'):\n",
    "            word = str(diff.strip('! '))\n",
    "            word_list.append(word)\n",
    "    final_word_list = \" \".join(word_list)\n",
    "    return final_word_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                root_hypernym TUI_ID  tree_level\n",
      "0                       event   T051         1.0\n",
      "1                    activity   T052         2.0\n",
      "2                    behavior   T053         3.0\n",
      "3                    behavior   T054         4.0\n",
      "4                    behavior   T055         4.0\n",
      "..                        ...    ...         ...\n",
      "122           group-attribute   T102         3.0\n",
      "123      intellectual-product   T170         3.0\n",
      "124                regulation   T089         4.0\n",
      "125            classification   T185         4.0\n",
      "126                  language   T171         3.0\n",
      "\n",
      "[127 rows x 3 columns]\n",
      "['event', '  activity', '    behavior', '      behavior', '      behavior', '    activity', '    work-activity', '      healthcare-activity', '        lab-procedure', '        test-procedure', '        medical-procedure', '      research', '        molecular-research', '      regulatory-act', '      educational-act', '    machine-process', '  process', '    injury', '    human-effect', '      human-effect', '    natural-process', '      natural-process', '        natural-process', '          organism-process', '            mental-process', '          organ-process', '          cell-process', '          molecular-process', '            gene-process', '        abnormal-process', '          disease', '            mental-disease', '            cancer-process', '          cellular-flaw', '          disease-model', 'entity', '  object', '    being', '      virus', '      bacteria', '      archaeon', '      eukaryote', '        plant', '        fungus', '        animal', '          vertebrate', '            amphibian', '            bird', '            fish', '            reptile', '            mammal', '              human', '     body-part', '      embryo-part', '      body-part', '        body-part t023', '        tissue', '        cell', '        cell-part', '        gene', '      body-defect', '        birth-defect', '        body-defect', '    object', '      medical-device', '        drug-device', '      research-device', '      medicine', '    substance', '      body-substance', '      chemical', '        chemical-structure', '          organic-chemical', '            nucleic-acid', '            protein', '          element', '          inorganic-chemical', '        chemical-functional', '          drug', '            antibiotic', '          biomedical-material', '          biological-substance', '            hormone', '            enzyme', '            vitamin', '            immunologic-factor', '            receptor', '          reagent', '          poison', '      food', '  concept', '    organism-attribute', '      clinical-attribute', '    finding', '      lab-result', '      symptom', '    concept', '      temporal-concept', '      quality', '      numerical', '      spatial-concept', '        body-region', '        body-space', '        geography', '        molecular-sequence', '          nucleotide-sequence', '          amino-acid-sequence', '          carbohydrate-sequence', '      function', '        body-system', '    discipline', '      biomedical-discipline', '    organization', '      health-care-organization', '      professional-society', '      relief-rganization', '    group', '      professional-group', '      population', '      family', '      age-group', '      patients', '    group-attribute', '    intellectual-product', '      regulation', '      classification', '    language']\n"
     ]
    }
   ],
   "source": [
    "# Create dataframe for umls semantic tree for least specific hypernyms - most complexity reduction\n",
    "columns = ['root_hypernym','TUI_ID','tree_level']\n",
    "semantic_df = pd.read_csv(\n",
    "    \"./datasets/modified_umls_semantic_type_tree.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    names=columns\n",
    ")\n",
    "semantic_wordphrases = semantic_df['root_hypernym'].values.tolist()\n",
    "semantic_wordphrases = [x.lower() for x in semantic_wordphrases]\n",
    "print(semantic_df)\n",
    "print(semantic_wordphrases)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/karl/anaconda3/envs/rapids-21.08/lib/python3.7/site-packages/scispacy/candidate_generation.py:284: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  extended_neighbors[empty_vectors_boolean_flags] = numpy.array(neighbors)[:-1]\n",
      "/home/karl/anaconda3/envs/rapids-21.08/lib/python3.7/site-packages/scispacy/candidate_generation.py:285: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  extended_distances[empty_vectors_boolean_flags] = numpy.array(distances)[:-1]\n"
     ]
    }
   ],
   "source": [
    "#test_document = \"an organothiophosphate insecticide.\"\n",
    "f = open(\"./datasets/definitions_output.txt\", \"a+\")\n",
    "for line in open(\"./datasets/UMLS/MRDEF.RRF\", 'r'):\n",
    "    #print(line)\n",
    "    document = str(line.strip('\\n'))\n",
    "    source = str(line.split('|')[4])\n",
    "    if source == 'MSH':\n",
    "        document = str(line.split('|')[5])\n",
    "        test_document = document\n",
    "        count = 0\n",
    "        new_document = test_document.lower()\n",
    "        new_document, cw_list = automatic_translation(test_document, new_document, find_complex_words, sub_umls_hypernym, semantic_df)\n",
    "        try:\n",
    "            new_document = runGinger(new_document)\n",
    "        except KeyError:\n",
    "            new_document = \"ERROR WITH GINGERIT - INVALID RESULT|\"+new_document\n",
    "        cw_new_list = find_complex_words((new_document))\n",
    "        og_fk_score, new_fk_score, og_fk_grade, new_fk_grade, og_dc_score, new_dc_score, og_cl_score, new_cl_score, og_ar_index, new_ar_index = grade_the_documents(test_document, new_document)\n",
    "        word_changes = sentencediff(test_document.lower(), new_document)\n",
    "        #orig text|sub text|words changed|cw orig|cw sub|orig fk|sub fk|orig grad|sub grade|orig dc|sub dc|orig cl|sub cl|orig ari|sub ari|\n",
    "        final_row = \"RESULT: {}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\".format(test_document.strip('\\n'), new_document.strip('\\n'), word_changes.strip('\\n'), og_fk_score,new_fk_score,og_fk_grade,new_fk_grade, og_dc_score,new_dc_score,og_cl_score,new_cl_score,og_ar_index,new_ar_index)\n",
    "        #f.write(final_row+'\\n')\n",
    "        f.write(final_row+'\\n')\n",
    "        count += 1\n",
    "\n",
    "\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "\n",
    "f.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#test_document = \"an organothiophosphate insecticide.\"\n",
    "f = open(\"./datasets/definitions_output.txt\", \"a+\")\n",
    "\n",
    "        new_document, cw_list = automatic_translation(test_document, new_document, find_complex_words, sub_umls_hypernym, semantic_df)\n",
    "        try:\n",
    "            new_document = runGinger(new_document)\n",
    "        except KeyError:\n",
    "            new_document = \"ERROR WITH GINGERIT - INVALID RESULT|\"+new_document\n",
    "        cw_new_list = find_complex_words((new_document))\n",
    "        og_fk_score, new_fk_score, og_fk_grade, new_fk_grade, og_dc_score, new_dc_score, og_cl_score, new_cl_score, og_ar_index, new_ar_index = grade_the_documents(test_document, new_document)\n",
    "        word_changes = sentencediff(test_document.lower(), new_document)\n",
    "        #orig text|sub text|words changed|cw orig|cw sub|orig fk|sub fk|orig grad|sub grade|orig dc|sub dc|orig cl|sub cl|orig ari|sub ari|\n",
    "        final_row = \"RESULT: {}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\".format(test_document.strip('\\n'), new_document.strip('\\n'), word_changes.strip('\\n'), og_fk_score,new_fk_score,og_fk_grade,new_fk_grade, og_dc_score,new_dc_score,og_cl_score,new_cl_score,og_ar_index,new_ar_index)\n",
    "        #f.write(final_row+'\\n')\n",
    "        f.write(final_row+'\\n')\n",
    "        count += 1\n",
    "\n",
    "\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "\n",
    "f.close()\n",
    "\n",
    "documents = []\n",
    "for line in open(\"./datasets/UMLS/MRDEF.RRF\", 'r'):\n",
    "    #print(line)\n",
    "    document = str(line.strip('\\n'))\n",
    "    source = str(line.split('|')[4])\n",
    "    if source == 'MSH':\n",
    "        document = str(line.split('|')[5])\n",
    "        test_document = document\n",
    "        documents.append(test_document)\n",
    "\n",
    "delayed_results = [delayed(automatic_translation)(document,new_document=document.lower(),find_complex_words, sub_umls_hypernym, semantic_df) for document in documents]\n",
    "final_results = compute(*delayed_results, scheduler=\"threads\")\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def tfidf_search(query, list_to_search):\n",
    "    # Get tf-idf matrix using fit_transform function\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(list_to_search) # Store tf-idf representations of all docs\n",
    "    query_vec = vectorizer.transform([query])  # Ip -- (n_docs,x), Op -- (n_docs,n_Feats)\n",
    "    results = cosine_similarity(X, query_vec).reshape((-1,))  # Op -- (n_docs,1) -- Cosine Sim with each doc\n",
    "    print(\"#####################################\",query,\"#####################################\")\n",
    "    print(X.shape) # (Number of wordphrases, Number of unique words)\n",
    "    print(\"###################################################################################\")\n",
    "    tfidif_results = results.argsort()[-1:][::-1]\n",
    "    return tfidif_results\n",
    "\n",
    "    #for i in results.argsort()[-1:][::-1]:\n",
    "        #print(df.iloc[i, 0], \"--\", df.iloc[i, 1],\"--\", df.iloc[i, 3])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Med7 to resolve medicines\n",
    "Need to figure out how to use two versions of spacy\n",
    "Can a python script switch environments?\n",
    "\n",
    "## Create hypernymy NER to make a training set, this will allow us to train a machine learning model that automatically subs hypernyms\n",
    "\n",
    "## Steal the nature paper methods to resolve abbreviations or figure out how to resolve them yourself\n",
    "\n",
    "# For v2:\n",
    "## Create to the point! - a module that takes an EHR note finds bottom line [diagnoses] and [plan] for such, tells patient they have [condition], which is a [definition], that they must do [plan] for.\n",
    "Do this with NER based rules. If entity == label_ then return that variable.\n",
    "\n",
    "Include a section called \"Why your doctor thinks you have this.\" Then create NER based rules that recognize subjective, physical exam findings etc.\n",
    "\n",
    "Thought of idea of all permutations and combinations, but again scorers really just grade based on number words, word length and sentence length.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}